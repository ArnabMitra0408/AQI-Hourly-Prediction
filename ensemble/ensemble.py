# -*- coding: utf-8 -*-
"""ensemble_2.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Vg9lR8Qq1XO_ZThc0AV3HkXb29-FMjn6
"""

import pandas as pd
import numpy as np
import torch
import torch.nn as nn
from torch.utils.data import Dataset, DataLoader
from sklearn.preprocessing import StandardScaler
from sklearn.ensemble import RandomForestRegressor
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_error, r2_score
import pickle
import os
import matplotlib.pyplot as plt

# from google.colab import drive
# drive.mount('/content/drive')

final_data_path = '/Users/niranjanens/data_mining/project/AQI-Hourly-Prediction/data_store/final_data/final_data.csv'
rf_model_path = '/Users/niranjanens/data_mining/project/AQI-Hourly-Prediction/ensemble_2/rf_model.pkl'
lstm_model_path = '/Users/niranjanens/data_mining/project/AQI-Hourly-Prediction/ensemble_2/lstm_model.pth'
test_predictions_path = '/Users/niranjanens/data_mining/project/AQI-Hourly-Prediction/ensemble_2/test_predictions.csv'
loss_plot_path = '/Users/niranjanens/data_mining/project/AQI-Hourly-Prediction/ensemble_2/lstm_loss_plot.png'
sequence_length = 24  # Use a 24-hour sequence for LSTM
test_size = 0.2

# Load dataset
data = pd.read_csv(final_data_path)
feature_cols = ['co', 'no', 'no2', 'o3', 'so2', 'pm2_5', 'pm10', 'nh3',
                'temperature_2m', 'relative_humidity_2m', 'rain',
                'wind_speed_10m', 'wind_direction_10m',
                'soil_temperature_0_to_7cm', 'soil_moisture_0_to_7cm']

X = data[feature_cols]
y = data['aqi']

# Split and scale data
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size, random_state=0)
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)
print(len(X_train_scaled))
print(len(X_test_scaled))

# Train or load Random Forest model
if os.path.exists(rf_model_path):
    print("Loading existing Random Forest model...")
    with open(rf_model_path, 'rb') as f:
        rf_model = pickle.load(f)
else:
    # Random Forest Model Training
    print("Training Random Forest model...")
    rf_model = RandomForestRegressor(n_estimators=200, max_depth=20, min_samples_split=5, n_jobs=-1, random_state=42)
    rf_model.fit(X_train_scaled, y_train)

    # Save the Random Forest model
    with open(rf_model_path, 'wb') as f:
        pickle.dump(rf_model, f)

# Predict with Random Forest on test data
rf_test_predictions = rf_model.predict(X_test_scaled)

print(len(rf_test_predictions))
print(len(y_test))

# LSTM Dataset for Direct Prediction
class AQIDataset(Dataset):
    def __init__(self, features, target, sequence_length=24):
        self.features = torch.FloatTensor(features)
        self.target = torch.FloatTensor(target.values.reshape(-1, 1))
        self.sequence_length = sequence_length

    def __len__(self):
        return len(self.features) - self.sequence_length

    def __getitem__(self, idx):
        X = self.features[idx:idx + self.sequence_length]
        y = self.target[idx + self.sequence_length]
        return X, y

# Define LSTM Model
class LSTMModel(nn.Module):
    def __init__(self, input_size, hidden_size=64, num_layers=2):
        super(LSTMModel, self).__init__()
        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)
        self.fc = nn.Linear(hidden_size, 1)

    def forward(self, x):
        lstm_out, _ = self.lstm(x)
        last_hidden = lstm_out[:, -1, :]
        out = self.fc(last_hidden)
        return out

# Prepare DataLoader for LSTM on AQI target directly
train_dataset = AQIDataset(X_train_scaled, y_train, sequence_length)
test_dataset = AQIDataset(X_test_scaled, y_test, sequence_length)

# Split the train dataset into training and validation sets
train_size = int(0.8 * len(train_dataset))
val_size = len(train_dataset) - train_size
train_dataset, val_dataset = torch.utils.data.random_split(train_dataset, [train_size, val_size])

train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)
val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)
test_loader = DataLoader(test_dataset, batch_size=1, shuffle=False)

# Train LSTM on AQI target directly with loss tracking
device = torch.device('mps' if torch.backends.mps.is_available() else 'cpu')
print(device)
lstm_model = LSTMModel(input_size=len(feature_cols)).to(device)
criterion = nn.MSELoss()
optimizer = torch.optim.Adam(lstm_model.parameters(), lr=0.001)

# Track losses for visualization
train_losses = []
val_losses = []

epochs = 50
for epoch in range(epochs):
    print(epoch)
    lstm_model.train()
    running_train_loss = 0.0
    for X_batch, y_batch in train_loader:
        X_batch, y_batch = X_batch.to(device), y_batch.to(device)
        optimizer.zero_grad()
        outputs = lstm_model(X_batch)
        loss = criterion(outputs, y_batch)
        loss.backward()
        optimizer.step()
        running_train_loss += loss.item()

    avg_train_loss = running_train_loss / len(train_loader)
    train_losses.append(avg_train_loss)

    # Validation loss calculation
    lstm_model.eval()
    running_val_loss = 0.0
    with torch.no_grad():
        for X_batch, y_batch in val_loader:
            X_batch, y_batch = X_batch.to(device), y_batch.to(device)
            outputs = lstm_model(X_batch)
            loss = criterion(outputs, y_batch)
            running_val_loss += loss.item()

    avg_val_loss = running_val_loss / len(val_loader)
    val_losses.append(avg_val_loss)

    if (epoch + 1) % 10 == 0:
        print(f"Epoch [{epoch+1}/{epochs}], Train Loss: {avg_train_loss:.4f}, Validation Loss: {avg_val_loss:.4f}")

# Save the trained LSTM model
torch.save(lstm_model.state_dict(), lstm_model_path)

# Plot training and validation losses
plt.figure(figsize=(10, 6))
plt.plot(train_losses, label="Training Loss")
plt.plot(val_losses, label="Validation Loss")
plt.xlabel("Epoch")
plt.ylabel("Loss")
plt.title("LSTM Training and Validation Loss")
plt.legend()
plt.savefig(loss_plot_path)
plt.close()

# Predict with LSTM on test data
lstm_model.eval()
lstm_predictions = []
with torch.no_grad():
    for X_batch, _ in test_loader:
        X_batch = X_batch.to(device)
        outputs = lstm_model(X_batch)
        lstm_predictions.append(outputs.cpu().item())

# Ensure rf_test_predictions and lstm_predictions are the same length
print(f"RF Predictions Length: {len(rf_test_predictions)}")
print(len(y_test))
rf_test_predictions = rf_test_predictions[sequence_length:]  # Remove initial predictions to align lengths
print(f"RF Predictions Length: {len(rf_test_predictions)}")
print(f"LSTM Predictions Length: {len(lstm_predictions)}")
final_predictions = 0.5 * np.array(rf_test_predictions) + 0.5 * np.array(lstm_predictions)

# Save predictions and evaluate
ensemble_results = pd.DataFrame({
    'RF_Predictions': rf_test_predictions,
    'LSTM_Predictions': lstm_predictions,
    'Final_Ensemble_Predictions': final_predictions
})
ensemble_results.to_csv(test_predictions_path, index=False)

mse = mean_squared_error(y_test[sequence_length:], final_predictions)
r2 = r2_score(y_test[sequence_length:], final_predictions)
print(f"Ensemble Model - MSE: {mse:.4f}, RÂ² Score: {r2:.4f}")

